{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: size\n",
      "6: shirt\n",
      "6: new\n",
      "6: black\n",
      "5: top\n",
      "5: pink\n",
      "4: bundle\n",
      "3: vs\n",
      "3: shoes\n",
      "3: os\n",
      "3: mascara\n",
      "3: lularoe\n",
      "3: leggings\n",
      "3: iphone\n",
      "3: hold\n",
      "3: crop\n",
      "3: bra\n",
      "3: baby\n",
      "2: xl\n",
      "2: women\n",
      "2: victoria\n",
      "2: sports\n",
      "2: silver\n",
      "2: shorts\n",
      "2: set\n",
      "2: secret\n",
      "2: rae\n",
      "2: plus\n",
      "2: one\n",
      "2: nwt\n",
      "2: nike\n",
      "2: jeans\n",
      "2: jacket\n",
      "2: horse\n",
      "2: girls\n",
      "2: faced\n",
      "2: face\n",
      "2: dunn\n",
      "2: case\n",
      "2: carrier\n",
      "2: car\n",
      "2: birthday\n",
      "2: beige\n",
      "2: beauty\n",
      "2: armour\n",
      "2: air\n",
      "1: sephora\n",
      "1: red\n",
      "1: qty\n",
      "1: purple\n",
      "1: puppy\n",
      "1: puffer\n",
      "1: pro\n",
      "1: print\n",
      "1: primer\n",
      "1: porcelain\n",
      "1: polka\n",
      "1: pokemon\n",
      "1: plated\n",
      "1: pi\n",
      "1: pet\n",
      "1: peplum\n",
      "1: patagonia\n",
      "1: partners\n",
      "1: pants\n",
      "1: pacific\n",
      "1: otterbox\n",
      "1: north\n",
      "1: necklace\n",
      "1: nautical\n",
      "1: nature\n",
      "1: mug\n",
      "1: mlb\n",
      "1: misty\n",
      "1: mists\n",
      "1: minnetonka\n",
      "1: mickey\n",
      "1: michaels\n",
      "1: michael\n",
      "1: metallic\n",
      "1: metal\n",
      "1: merry\n",
      "1: melville\n",
      "1: melissa\n",
      "1: medium\n",
      "1: maternity\n",
      "1: mask\n",
      "1: maroon\n",
      "1: marble\n",
      "1: maran\n",
      "1: makeup\n",
      "1: macaroons\n",
      "1: luminess\n",
      "1: lululemon\n",
      "1: lots\n",
      "1: lot\n",
      "1: long\n",
      "1: littlest\n",
      "1: listing\n",
      "1: acacia\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time as time\n",
    "# Import the pandas package, then use the \"read_csv\" function to read the labeled training data\n",
    "import pandas as pd\t\t# Allows to import the data\n",
    "from bs4 import BeautifulSoup \t# Will be used to remove the HTML characters\n",
    "import re\t\t\t# This packages allows to remove punctuation and numbers\n",
    "import nltk\t\t\t# Allows to remove the stopwords (those words that carry not meaning, like 'and', 'the'...)\n",
    "#nltk.download('all')\t\t# Downloads the stopwords data sets\n",
    "#print stopwords.words(\"english\")\n",
    "from nltk.corpus import stopwords\t# Import a stopword list in English\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\t\t# Allows us to use bag-of-words learning and vectorize the set\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# This is a problem of regression, therefore we need to use a regression package from scikit learn, and we are going to use a linear model at first.\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# For some reason python3 does not understand sklearn, Python2 does.\n",
    "#Also, Python2 have troubles with nltk. It may be due to an update of scipy that was not compatible with nltk anymore. I am now updating scipy and that solved the problem.\n",
    "\n",
    "\n",
    "# Imports the data. The target is the sentiment\n",
    "def train_data_import(data = \"train.tsv\"):\n",
    "\treturn pd.read_csv(data, header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "def test_data_import(data = \"test.tsv\"):\n",
    "\treturn pd.read_csv(data, header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "\n",
    "\n",
    "# The following function takes in some text and outputs a cleaned text\n",
    "def clean_review( raw_text ):\t\t# Removes the HTML, punctuation, numbers, stopwords...\n",
    "\trm_html = BeautifulSoup(raw_text).get_text()\t# removes html\n",
    "\tletters_only = re.sub(\"[^a-zA-Z]\",           \t# The pattern to search for; ^ means NOT\n",
    "                   \t\t  \" \",                   \t# The pattern to replace it with\n",
    "                          rm_html )              \t# The text to search\n",
    "\tlower_case = letters_only.lower()\t         \t# Convert to lower case\n",
    "\twords = lower_case.split()          \t     \t# Split into words\n",
    "\tstops = stopwords.words(\"english\")\n",
    "\tstops.append('ve')\n",
    "\tstops = set(stops)\n",
    "#\tenglish_words = words.words()[1:100]\n",
    "\tmeaningful_words = [w for w in words if not w in stops]\t# Remove stop words from \"words\"\n",
    "\treturn ' '.join(meaningful_words)\t\t\t# Joins the words back together separated by a space\n",
    "\n",
    "# The following function iterates clean_review over all reviews in the set\n",
    "def clean_all_reviews( raw_train_data, N_articles ):\n",
    "\tcleaned_reviews = []\n",
    "\tfor i in xrange(N_articles):\n",
    "\t\tcleaned_reviews.append(clean_review(raw_train_data[\"name\"][i]))\n",
    "\t\tif ( (i+1) % 1000 == 0 ):\n",
    "\t\t\tprint(' -- Clean review # %d' % i)\n",
    "\treturn cleaned_reviews\n",
    "\n",
    "\n",
    "# Look for: panda apply\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "def Bag_of_Words(cleaned_reviews, n_features = 100):\n",
    "\tvectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,    \t# Allows to tokenize\n",
    "                             preprocessor = None, \t# Allows to do some preprocessing\n",
    "                             stop_words = None,   \t# We could remove stopwords from here\n",
    "                             max_features = n_features) \t# Chooses a given number of words, just a subset of the huge total number of words.\n",
    "\t# fit_transform() does two functions: First, it fits the model\n",
    "\t# and learns the vocabulary; second, it transforms our training data\n",
    "\t# into feature vectors. The input to fit_transform should be a list of \n",
    "\t# strings.\n",
    "\tdata_features = vectorizer.fit_transform(cleaned_reviews)\n",
    "\tdata_features = data_features.toarray()\n",
    "\treturn data_features, vectorizer\n",
    "\n",
    "\n",
    "# Counts the words that appear in the reviews\n",
    "def word_count():\n",
    "\tdf_train = train_data_import()\n",
    "\tN_articles = 100#len(raw_train_data[\"review\"][:])\n",
    "\tcleaned_reviews = clean_all_reviews(df_train, N_articles)\n",
    "\ttrain_data_features, vectorizer = Bag_of_Words(cleaned_reviews, N_articles)\n",
    "\tvocab = vectorizer.get_feature_names()\n",
    "\tdist = np.sum(train_data_features, axis=0)\n",
    "\tword_count = sorted(zip(dist,vocab),reverse = True)\n",
    "\tfor count, tag in word_count:\n",
    "\t    print '{}: {}'.format(count, tag)\n",
    "\n",
    "word_count()\n",
    "\n",
    "def Trainer():\n",
    "\tTstart = time.time()\n",
    "\tprint '- Import all training reviews'\n",
    "\tdf_train = train_data_import()\n",
    "\tN_articles = 100000 #len(df_train[\"name\"])\n",
    "\tprint '- Start cleaning the training reviews'\n",
    "\tcleaned_reviews = clean_all_reviews(df_train, N_articles)\n",
    "\tprint '- Creating the bag-of-words with %d articles' % N_articles\n",
    "\ttrain_data_features, vectorizer = Bag_of_Words(cleaned_reviews)\n",
    "\tprint '- Trains a classifier'\n",
    "\tX_train, X_valid, Y_train, Y_valid = cross_validation.train_test_split(train_data_features, df_train[\"price\"][:N_articles], train_size = 0.8, random_state=10)\n",
    "#\tN_train = int(0.8 * N_articles)\n",
    "#\tX_train = train_data_features[:N_train]\n",
    "#\tY_train = df_train[\"price\"][:N_train]\n",
    "#\tX_valid = train_data_features[N_train:N_articles]\n",
    "#\tY_valid = df_train[\"price\"][N_train:N_articles]\n",
    "\tclf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\tforest = clf.fit( X_train, Y_train )\n",
    "\tY_predicted = clf.predict(X_valid)\n",
    "\trms = sqrt(mean_squared_error(Y_valid, Y_predicted))\t#The Kaggle competition uses rms as the metric\n",
    "#\tscore = clf.score(X_valid, Y_valid)\n",
    "\tTend = time.time()-Tstart\n",
    "\tprint '- Finished training in %f s, score = %f' % (Tend, rms)\n",
    "#\treturn train_data_features, score\n",
    "\n",
    "\n",
    "#df_train = train_data_import()\n",
    "\n",
    "#print(len(df_train[\"category_name\"]))\n",
    "\n",
    "#word_count()\n",
    "\n",
    "#Trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
